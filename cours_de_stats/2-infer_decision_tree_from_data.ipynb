{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux modèles d'arbres\n",
    "\n",
    "_Objectif du cours_:\n",
    "  - savoir ce qu'est un *arbre de décision*\n",
    "  - savoir en construire un à la main en partant d'un petit jeu de données grâce à l'algorithme basé sur le gain d'information par entropie\n",
    "  - savoir en construire un en utilisant une solution clé en mains de `scikit-learn`. Savoir interpréter les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "* [0. Jeu préparatoire: Qui est-ce?](#Jeu_preparatoire)\n",
    "* [1. Définition d'un arbre de décision](#Definition_arbre_de_decision)\n",
    "* [2. Comment mesurer l'information? Modèle d'entropie de Claude Shannon](#Entropy_model)\n",
    "  * [2.0. Exercice préparatoire : exemple de la détection de mail spam](#Entropy_model)\n",
    "  * [2.1. Définition de l'entropie](#Entropy_model)\n",
    "    * [2.1.0. Intuition pour un jeu de cartes](#Entropy_model)\n",
    "    * [2.1.1. Formule mathématique](#Formula)\n",
    "    * [2.1.2. Représentation de la courbe pour le cas binaire](#Binary_case)\n",
    "  * [2.2. Gain d'information](#Gain_Information)\n",
    "    * [2.2.0. Exercice](#Gain_Information)\n",
    "    * [2.2.0. Algorithme](#Algorithme)\n",
    "      * [Exercice 1: cas de variables binaires](#Algorithme)\n",
    "      * [Exercice 2: cas de variables non binaires](#Algorithme)\n",
    "* [3. Forêts aléatoires](#Forets)\n",
    "  * [3.0. Définition et Algorithme](#Forets)\n",
    "  * [3.1. Exercice](#Forets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Jeu_preparatoire\"></a>\n",
    "## 0. Jeu préparatoire: `Qui est-ce?`\n",
    "\n",
    "On va commencer par un jeu `Qui est-ce?`. Les règles du jeu sont:\n",
    "- 2 joueurs\n",
    "- 1 joueur choisit une carte avec l'image d'un personnage dans un paquet de cartes:\n",
    "\n",
    "![people_guess_who.png](../images/people_guess_who.png)\n",
    "\n",
    "qui peuvent être représentées numériquement par le tableau:\n",
    "\n",
    "| Est-un homme? | As de long cheveux? | Porte des lunettes? | Nom  |\n",
    "|-----|-----------|---------|-------|\n",
    "| Oui | Non        | Oui     | Brian |\n",
    "| Oui | Non        | Non      | John  |\n",
    "| Non  | Oui       | Non      | Aphra |\n",
    "| Non  | Non        | Non      | Aoife |\n",
    "\n",
    "- L'autre joueur :\n",
    "  * essaie de deviner quel personnage est sur la carte en posant une série de questions. La réponse est binaire c'est-à-dire que l'adversaire répond seulement par `oui` ou `non`.\n",
    "  * il gagne en devinant qui est sur la carte dans un `petit nombre de questions` et perd sinon\n",
    "\n",
    "\n",
    "_Exemple d'une série de questions commençant par `La personne porte-t-elle des lunettes?`:_\n",
    "\n",
    "![does_the_person_wear_glasses.png](../images/does_the_person_wear_glasses.png)\n",
    "\n",
    "1. En commençant par cette question, quel est le nombre moyen de questions posées par partie?\n",
    "\n",
    "Quel est l'arbre de décision optimal?\n",
    "\n",
    "![optimal_tree_guess_who.png](../images/optimal_tree_guess_who.png)\n",
    "2. Reprendre le jeu avec cette série de questions. Quel est le nombre moyen de questions posées par partie ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Definition_arbre_de_decision\"></a>\n",
    "## 1. Définition d'un arbre de décision\n",
    "\n",
    "Terminologie:\n",
    "\n",
    "![Decision-Tree terminology](../images/decision_tree_nodes.png)\n",
    "\n",
    "* `racine`: ...\n",
    "* `noeud interne`: ...\n",
    "* `feuille`: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Entropy_model\"></a>\n",
    "## 2. Comment mesurer l'information? Modèle d'`entropie` de _Claude Shannon_\n",
    "\n",
    "Comment capturer mathématiquement l'*information contenue dans une variable descriptive*?\n",
    "### 2.0. Exercice préparatoire : exemple de la `détection de mail spam`\n",
    "\n",
    "| ID  |MOTS SUSPECTS | EXPEDITEUR INCONNU | CONTIENT DES IMAGES | CATEGORIE |\n",
    "|-----|-------|--------|--------|----------|\n",
    "| 376 | Vrai  | Faux  | Vrai   | spam     |\n",
    "| 489 | Vrai  | Vrai   | Faux  | spam     |\n",
    "| 541 | Vrai  | Vrai   | Faux  | spam     |\n",
    "| 693 | Faux | Vrai   | Vrai   | normal   |\n",
    "| 782 | Faux | Faux  | Faux  | normal   |\n",
    "| 976 | Faux | Faux  | Faux  | normal   |\n",
    "\n",
    "Le jeu de données ci-dessus contient 3 variables descriptives binaires:\n",
    "* `MOTS SUSPECTS` est vrai si un courriel contient un ou plusieurs mots que l'on trouve généralement dans les e-mails de spam (par exemple, casino, viagra, banque ou compte)\n",
    "* `EXPEDITEUR INCONNU` est vrai si l'e-mail provient d'une adresse qui n'est pas répertoriée dans les\n",
    "dans les contacts de la personne qui a reçu l'e-mail\n",
    "* `CONTIENT DES IMAGES` est vrai si l'e-mail contient une ou plusieurs images. vrai si l'e-mail contient une ou plusieurs images.\n",
    "\n",
    "Exemples d'arbres de décision cohérents avec l'échantillon (expliquez pourquoi):\n",
    "\n",
    "![spam_detection_example_of_decision_trees.png](../images/spam_detection_example_of_decision_trees.png)\n",
    "\n",
    "### 2.1. Définition de l'entropie\n",
    "\n",
    "C'est une mesure informatique de l'impureté d'un ensemble.\n",
    "\n",
    "#### 2.1.0. Intuition pour un jeu de cartes\n",
    "C'est l'incertitude associée au fait de deviner le résultat d'une sélection aléatoire dans le jeu. Par exemple, \n",
    "* si vous deviez sélectionner au hasard une dans l'ensemble de la figure (a), votre incertitude serait nulle (Pourquoi? R/ car vous seriez certain de choisir une carte.) Cet ensemble présente donc\n",
    "zéro entropie. \n",
    "* Si, par contre, vous deviez choisir au hasard un élément de l'ensemble vous seriez très incertain quant à toute prédiction, (Pourquoi?) C'est pourquoi cet ensemble a une entropie très élevée.\n",
    "\n",
    "![entropie_jeu_de_cartes.png](../images/entropie_jeu_de_cartes.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from summer.model.tree import entropy_for_binary_variable, entropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGSIZE = (15, 5)\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropie_jeu_de_cartes_a = entropy_for_binary_variable(1)\n",
    "np.testing.assert_allclose(entropie_jeu_de_cartes_a, 0)\n",
    "\n",
    "probabilities = [1/12 for i in range(0, 12)]\n",
    "entropie_jeu_de_cartes_f = entropy(probabilities)\n",
    "np.testing.assert_allclose(entropie_jeu_de_cartes_f, 3.585, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Question_: Calculer l'entropie du jeu (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Formula\"></a>\n",
    "#### 2.1.1. Formule mathématique\n",
    "\n",
    "L'équation suivante est la pierre angulaire de la théorie moderne de l'information et est une excellente mesure de l'impureté càd de l'hétérogénéité, d'un ensemble, d'une population.\n",
    "\\begin{equation}\n",
    "H(t)  = -\\sum_{i = 1}^{l} P(t=i) * log_{s}(P(t=i))\n",
    "\\end{equation}\n",
    "où:\n",
    "* $t$ = \"target feature\", la variable cible\n",
    "* $i$ = une catégorie de la variable cible\n",
    "* $P(t=i)$ est la probabilité que le résultat de la sélection aléatoire d'un élément $t$ soit du type $i$.\n",
    "* $l$ est le nombre de types différents de choses dans l'ensemble,\n",
    "* $s$ est une base logarithmique arbitraire. On la prend  historiquement égale à 2, pour obtenir un résultat en nombre de bits.\n",
    "\n",
    "Quelle est l'intuition derrière la formule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Binary_case\"></a>\n",
    "#### 2.1.2. Représentation de la courbe pour le cas binaire\n",
    "\\begin{align*}\n",
    "p &= P(t=1) \\\\\n",
    "H(t)  &= - p * log_{2}(p) - (1 - p) * log_{2}(1 - p)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_probability, max_probability = 0, 1\n",
    "n_steps = 100\n",
    "probabilities = np.linspace(min_probability, max_probability, n_steps)\n",
    "entropies = [\n",
    "    entropy_for_binary_variable(probability) for probability in probabilities\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_entropy_coordinates = {\"x\": 0.5, \"y\": 1}\n",
    "kwargs = dict(linestyle=\"--\", color=\"red\")\n",
    "if VERBOSE:\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.scatter(x=probabilities, y=entropies)\n",
    "    ax.axvline(opt_entropy_coordinates[\"x\"], **kwargs)\n",
    "    ax.axhline(opt_entropy_coordinates[\"y\"], **kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Gain_Information\"></a>\n",
    "## 2.2. Gain d'information\n",
    "_Quelle est la relation entre une mesure de l'hétérogénéité d'un ensemble et l'analyse prédictive ?_ \n",
    "\n",
    "Si on peut construire une séquence de tests qui divise les données d'apprentissage en ensembles purs par rapport aux valeurs des caractéristiques cibles, nous pouvons alors étiqueter un individu en \n",
    "* lui appliquant la même séquence de tests et en \n",
    "* l'étiquetant avec la valeur de la caractéristique cible des instances dans l'ensemble où elle se trouve.\n",
    "\n",
    "#### 2.2.1. Exercice\n",
    "Revenons au jeu de données d'emails. Quelle 1ere variable choisir pour patitionner? \n",
    "\n",
    "Indications: \n",
    "* C'est la variable qui discriminerait le mieux entre un spam et un mail normal. \n",
    "* Voici comment les instances de l'ensemble de données de spam se répartissent lorsque nous utilisons chacune des variables descriptives de l'ensemble de données de spam.\n",
    "\n",
    "![entropie_jeu_de_cartes.png](../images/feature_selection.png)\n",
    "\n",
    "Modèle (lien intuition - entropie): c'est la variable qui réduit le plus l'entropie globale \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Algorithme\"></a>\n",
    "#### 2.2.1. Algorithme\n",
    "* 1. Calculer l'entropie de l'ensemble de données original par rapport à la caractéristique cible. Cela nous donne une mesure de la quantité d'information nécessaire pour organiser l'ensemble de données en ensembles purs.\n",
    "\\begin{align*}\n",
    "H(t, \\mathcal{D})  &= -\\sum_{l \\in levels(t)} P(t=i) * log_{s}(P(t=i)) \\\\\n",
    "\\end{align*}\n",
    "* 2. Pour chaque caractéristique descriptive, créez les ensembles qui résultent de la partition des instances de l'ensemble de données en utilisant leurs caractéristiques. Puis additionnez les scores d'entropie de chacun de ces ensembles. Cela donne une mesure de l'information qui\n",
    "nécessaires pour organiser les instances en ensembles purs après les avoir divisées à l'aide de la caractéristique descriptive.\n",
    "\\begin{align*}\n",
    "rem(d, \\mathcal{D}) &= \\sum_{l \\in levels(t)} \\frac{|\\mathcal{D}_{d=l}|}{|\\mathcal{D}|} * H(t, \\mathcal{D}_{d=l}) \\\\\n",
    "\\end{align*}\n",
    "* 3. Soustraire la valeur d'entropie restante (calculée à l'étape 2) de la valeur d'entropie\n",
    "d'origine (calculée à l'étape 1) pour obtenir le gain d'information.\n",
    "\\begin{align*}\n",
    "IG(d, \\mathcal{D})  &= H(t, \\mathcal{D}) - rem(d, \\mathcal{D}) \n",
    "\\end{align*}\n",
    "\n",
    "#### Exercice 1: cas variables binaires\n",
    "Implémenter cet algorithme sur le dataset spam.\n",
    "\n",
    "#### Exercice 2: cas variables non binaires\n",
    "Implémenter cet algorithme sur un dataset de végétation.\n",
    "\n",
    "| ID | STREAM | SLOPE    | ELEVATION | VEGETATION |\n",
    "|----|--------|----------|-----------|------------|\n",
    "| 1  | FALSE  | steep    | high      | chapparal  |\n",
    "| 2  | TRUE   | moderate | low       | riparian   |\n",
    "| 3  | TRUE   | steep    | medium    | riparian   |\n",
    "| 4  | FALSE  | steep    | medium    | chapparal  |\n",
    "| 5  | FALSE  | flat     | high      | conifer    |\n",
    "| 6  | TRUE   | steep    | highest   | conifer    |\n",
    "| 7  | TRUE   | steep    | high      | chapparal  |\n",
    "\n",
    "\n",
    "Quel est l'arbre inféré?\n",
    "\n",
    "R/ \n",
    "![final_tree_for_vegetation_dataset.png](../images/final_tree_for_vegetation_dataset.png)\n",
    "\n",
    "Quelle prédiction aurait-on pour cet individu? \n",
    "\n",
    "| ID | STREAM | SLOPE    | ELEVATION | VEGETATION |\n",
    "|----|--------|----------|-----------|------------|\n",
    "| 8  | TRUE   | moderate | high      | chapparal  |\n",
    "\n",
    "_Remarque_: il n'est pas dans l'ensemble d'apprentissage. Donc nous généralisons la décision, au-delà du jeu d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3\n",
    "Comparer avec le résultat donné par `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix,\n",
    "    plot_confusion_matrix, \n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from summer.preprocessing.encoding import encode_ordinal\n",
    "from summer.tools.common_path import ROOT_PATH\n",
    "\n",
    "from cloud_io.gcp.io import download_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"VEGETATION\"\n",
    "FEATURES_COLS = [\"STREAM\", \"SLOPE\", \"ELEVATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"decision_tree_course\"\n",
    "blob_name = \"data/vegetation.csv\"\n",
    "download_path = Path(ROOT_PATH, blob_name)\n",
    "\n",
    "# Download file\n",
    "\n",
    "download_path.unlink(missing_ok=True)\n",
    "assert not download_path.exists()\n",
    "\n",
    "DOWNLOAD_KWARGS = dict(\n",
    "    bucket_as_local=ROOT_PATH,\n",
    "    bucket_name=BUCKET_NAME,\n",
    ")\n",
    "download_path_out = download_file(download_path, **DOWNLOAD_KWARGS)\n",
    "\n",
    "# Read file\n",
    "df = pd.read_csv(\n",
    "    download_path_out,\n",
    "    index_col=\"ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_shape = (7, 4)\n",
    "np.testing.assert_allclose(df.shape, expected_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[FEATURES_COLS], df[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"STREAM\": {False: 0, True: 1},\n",
    "    \"SLOPE\": {\"flat\": 0, \"moderate\": 1, \"steep\": 2,},\n",
    "    \"ELEVATION\": {\"low\": 0, \"medium\": 1, \"high\": 2, \"highest\": 3},\n",
    "}\n",
    "X_preprocessed = encode_ordinal(X, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_kwargs = dict(random_state=0)\n",
    "classifier = DecisionTreeClassifier(criterion=\"entropy\", **tree_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_preprocessed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERBOSE:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=FIGSIZE)\n",
    "    plot_tree(classifier, feature_names=FEATURES_COLS, ax=ax) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Forets\"></a>\n",
    "## 3. Forêts aléatoires\n",
    "\n",
    "### 3.0. Définition et Algorithme\n",
    "* 1. Bagging = **B**ootstrap **Agg**regat**ing** avec des arbres de décisions: \n",
    "  * chaque modèle de l'ensemble est entraîné sur un échantillon aléatoire de l'ensemble de données.\n",
    "ensemble est entraîné sur un _échantillon aléatoire de l'ensemble de données_. Important! chaque échantillon aléatoire est de la même taille que l'ensemble de données et l'échantillonnage avec remplacement ou **tirage avec remise** est utilisé. On appelle ces échantillons aléatoires, _échantillons bootstrap_. \n",
    "  * un modèle est induit à partir de chaque échantillon bootstrap. Donc ils sont tous différents.\n",
    "* 2. Échantillonnage du sous-espace des variables: l'échantillonnage est étendu de manière à ce que chaque échantillon bootstrap n'utilise qu'un sous-ensemble aléatoire des variables. Cet échantillonnage de l'ensemble des caractéristiques encourage davantage la diversité des arbres au sein de l'ensemble.\n",
    "ensemble et présente l'avantage de réduire le temps d'apprentissage de chaque arbre.\n",
    "* 3. la prédiction finale s'obtient en par un **vote à la majorité**\n",
    "\n",
    "\n",
    "\n",
    "### 3.1. Exercice\n",
    "Le tableau suivant présente un jeu de données contenant les détails de 5 participants à une étude sur les maladies cardiaques, et une caractéristique cible `RISQUE DE MALADIE DU COEUR` qui décrit leur risque de maladie cardiaque. Chaque patient est décrit en fonction de 5 caractéristiques descriptives binaires :\n",
    "* `EXERCICE PHYSIQUE` : à quelle fréquence fait-il de l'exercice ?\n",
    "* `FUMEUR` : fume-t-il ?\n",
    "* `OBESE` : est-il en surpoids ?\n",
    "* `FAMILLE`: est-ce que l'un de leurs parents ou frères et soeurs a souffert d'une maladie cardiaque ?\n",
    "\n",
    "| ID | EXERCICE PHYSIQUE | FUMEUR | OBESE | FAMILLE | RISQUE DE MALADIE DU COEUR |\n",
    "|----|----------|--------|-------|--------|---------------------|\n",
    "| 1  | journalier    | Faux  | Faux | Oui    | faible                 |\n",
    "| 2  | hebdomadaire   | Vrai   | Faux | Oui    | élevé                |\n",
    "| 3  | journalier    | Faux  | Faux | Non     | faible                 |\n",
    "| 4  | rare   | Vrai   | Vrai  | Oui    | élevé                |\n",
    "| 5  | rare   | Vrai   | Vrai  | Non     | élevé                |\n",
    "\n",
    "a) Dans le cadre de l'étude, les chercheurs ont décidé de créer un modèle prédictif pour sélectionner les participants, en fonction de leur risque de maladie cardiaque. Il vous a été demandé d'implémenter ce modèle de dépistage en utilisant une **Forêt aléatoire**.\n",
    "\n",
    "Les 3 tableaux ci-dessous répertorient 3 échantillons bootstrap qui ont été générés à partir de l'ensemble de données ci-dessus. En utilisant ces échantillons bootstrap, créez les arbres de décision qui seront dans le modèle Forêt aléatoire (utilisez le gain d'information basé sur l'entropie comme critère de sélection des variables).\n",
    "\n",
    "| ID | EXERCICE PHYSIQUE | FAMILLE | RISQUE DE MALADIE DU COEUR |\n",
    "|----|----------|--------|---------------------|\n",
    "| 1  | journalier    | Oui    | faible                 |\n",
    "| 2  | hebdomadaire   | Oui    | élevé                |\n",
    "| 3  | journalier    | Non     | faible                 |\n",
    "| 4  | rare   | Oui    | élevé                |\n",
    "| 5  | rare   | Non     | élevé                |\n",
    "\n",
    "| ID | EXERCICE PHYSIQUE| OBESE | RISQUE DE MALADIE DU COEUR |\n",
    "|----|--------|-------|---------------------|\n",
    "| 1  | Faux  | Faux | faible                 |\n",
    "| 2  | Vrai   | Faux | élevé                |\n",
    "| 3  | Faux  | Faux | faible                 |\n",
    "| 4  | Vrai   | Vrai  | élevé                |\n",
    "| 5  | Vrai   | Vrai  | élevé                |\n",
    "\n",
    "| ID | OBESE | FAMILLE | RISQUE DE MALADIE DU COEUR |\n",
    "|----|-------|--------|---------------------|\n",
    "| 1  | Faux | Oui    | faible                 |\n",
    "| 2  | Faux | Oui    | élevé                |\n",
    "| 3  | Faux | Non     | faible                 |\n",
    "| 4  | Vrai  | Oui    | élevé                |\n",
    "| 5  | Vrai  | Non     | élevé                |\n",
    "\n",
    "b) En supposant que le modèle RF que vous avez créé utilise le vote majoritaire, quelle prédiction donnera-t-il pour la requête suivante ?\n",
    "\n",
    "| ID | EXERCICE PHYSIQUE | FUMEUR | OBESE | FAMILLE |\n",
    "|----|----------|--------|-------|--------|\n",
    "| 1  | rare    | Faux  | Vrai | Oui    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "1. _Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies_, Book by Aoife D'Arcy, Brian Mac Namee, and John D. Kelleher. Chapter 4 Information-based Learning. \n",
    "  * _Lien web pour extrait Arbres_:\n",
    "https://machinelearningbook.com/wp-content/uploads/2015/07/FMLPDA_SampleChapter_InformationBasedLearning.pdf\n",
    "  * _Livre en entier_: https://dokumen.pub/qdownload/fundamentals-of-machine-learning-for-predictive-data-analytics-algorithms-worked-examples-and-case-studies-1-edition-july-24-2015-978-0262029445.html\n",
    "2. Solutions: http://machinelearningbook.com/wp-content/uploads/2015/07/FMLPDA_freely_avail_solutions.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# End of script\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "620a642af2e4c3efb67df8943057aad9515b3766a7765ba9ef1f471869f3567c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
